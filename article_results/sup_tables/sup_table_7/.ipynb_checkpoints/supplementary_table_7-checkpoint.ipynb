{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from comet_ml import Experiment\n",
    "import numpy as np\n",
    "from numpy import inf, ndarray\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import re\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model, Model\n",
    "from tempfile import TemporaryFile\n",
    "from keras import layers\n",
    "from keras.callbacks import History, ReduceLROnPlateau\n",
    "from keras.layers import Input, BatchNormalization, Activation\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Dropout, Layer\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.regularizers import l2\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from copy import deepcopy\n",
    "from math import ceil\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Define custom metrics for evaluation\n",
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "def get_cindex(y_true, y_pred):\n",
    "    g = tf.subtract(tf.expand_dims(y_pred, -1), y_pred)\n",
    "    g = tf.cast(g == 0.0, tf.float32) * 0.5 + tf.cast(g > 0.0, tf.float32)\n",
    "\n",
    "    f = tf.subtract(tf.expand_dims(y_true, -1), y_true) > 0.0\n",
    "    f = tf.matrix_band_part(tf.cast(f, tf.float32), -1, 0)\n",
    "\n",
    "    g = tf.reduce_sum(tf.multiply(g, f))\n",
    "    f = tf.reduce_sum(f)\n",
    "\n",
    "    return tf.where(tf.equal(g, 0), 0.0, g/f)\n",
    "\n",
    "def pearson_r(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x, axis=0)\n",
    "    my = K.mean(y, axis=0)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = K.sum(xm * ym)\n",
    "    x_square_sum = K.sum(xm * xm)\n",
    "    y_square_sum = K.sum(ym * ym)\n",
    "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
    "    r = r_num / r_den\n",
    "    return K.mean(r)\n",
    "\n",
    "def mse_sliced(th):\n",
    "    def mse_similars(y_true,y_pred):\n",
    "        condition = K.tf.math.less_equal(y_pred,th)\n",
    "        indices = K.tf.where(condition)\n",
    "        slice_true = K.tf.gather_nd(y_true,indices)\n",
    "        slice_pred = K.tf.gather_nd(y_pred,indices)\n",
    "        mse_sliced = K.mean(K.square(slice_pred - slice_true), axis=-1)\n",
    "        return mse_sliced\n",
    "    return mse_similars\n",
    "\n",
    "#Model evaluation function\n",
    "def model_evaluate(y_pred,Y_cold,thresh,df_cold):\n",
    "    true = np.reshape(Y_cold,len(df_cold))\n",
    "    pred = np.reshape(y_pred,len(df_cold))\n",
    "    cor = np.corrcoef(true,pred)\n",
    "    mse_all = sklearn.metrics.mean_squared_error(true,pred)\n",
    "    # calculate mse of similars\n",
    "    if (len(pred[np.where(pred<=thresh)])>0):\n",
    "        mse_sims = sklearn.metrics.mean_squared_error(true[pred<=thresh],pred[pred<=thresh])\n",
    "    else:\n",
    "        mse_sims = \"None\"\n",
    "    # turn to categorical to calculate precision and accuracy\n",
    "    true_cat = true <= thresh\n",
    "    pred_cat = pred <= thresh\n",
    "    pos = np.sum(pred_cat)\n",
    "    if (len(pred[np.where(pred<=thresh)])>0):\n",
    "        prec = precision_score(true_cat,pred_cat)\n",
    "    else: \n",
    "        prec = \"None\"\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(true_cat,pred_cat)\n",
    "    result =pd.DataFrame({'cor' : cor[0,1], 'mse_all' : mse_all, 'mse_similars' : mse_sims,'precision': prec, 'accuracy': acc,\n",
    "                         'positives' : pos}, index=[0])\n",
    "    return(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepSIBA A375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.536088  0.008665       0.00481        1.0  0.831968   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.016438        133  ,\n",
       "    Unnamed: 0       cor  mse_all  mse_similars  precision  accuracy    recall  \\\n",
       " 0           0  0.576997  0.00841      0.004198   0.891473  0.828167  0.013958   \n",
       " \n",
       "    positives  \n",
       " 0        129  ,\n",
       "    Unnamed: 0      cor   mse_all  mse_similars  precision  accuracy    recall  \\\n",
       " 0           0  0.57376  0.008617      0.005039   0.796296  0.830722  0.021117   \n",
       " \n",
       "    positives  \n",
       " 0        216  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.552466  0.008411      0.007144   0.948905  0.826541   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.015591        137  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.536985  0.009289      0.006721    0.96875  0.829476   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.029822        256  ]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a375 = []\n",
    "for i in range(5):\n",
    "    perf = pd.read_csv(\"C:/Users/user/Documents/deepSIBA/article_results/supplementary_table_data/sup_table_7/a375/fold_%s/ensemble_performance.csv\"%i)\n",
    "    a375.append(perf)\n",
    "a375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepSIBA VCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.546593  0.025226      0.010298   0.617834  0.853172   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.008812        157  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.553764  0.024963      0.005662   0.720721  0.852074   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.007206        111  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.542301  0.025575      0.002271        0.8  0.848983   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.005648         80  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.527558  0.026298      0.004757   0.460317  0.849933   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.010364        252  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.524739  0.026194      0.004728   0.615385  0.853573   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.002192         39  ]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcap = []\n",
    "for i in range(5):\n",
    "    perf = pd.read_csv(\"C:/Users/user/Documents/deepSIBA/article_results/supplementary_table_data/sup_table_7/vcap/fold_%s/ensemble_performance.csv\"%i)\n",
    "    vcap.append(perf)\n",
    "vcap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepSIBA PC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   Unnamed: 0    cor   mse_all  mse_similars  precision  accuracy    recall  \\\n",
       " 0           0  0.554  0.010968      0.007119   0.948718  0.857442  0.005309   \n",
       " \n",
       "    positives  \n",
       " 0         39  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.568278  0.010793      0.012922   0.958333  0.860033   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.003367         24  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.495685  0.011748      0.010654   0.916667  0.859704   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.004815         36  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.540541  0.011014      0.009904        1.0  0.857915   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.003604         25  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.527495  0.011175      0.009163        1.0  0.859313   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.006677         46  ]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc3 = []\n",
    "for i in range(5):\n",
    "    perf = pd.read_csv(\"C:/Users/user/Documents/deepSIBA/article_results/supplementary_table_data/sup_table_7/pc3/fold_%s/ensemble_performance.csv\"%i)\n",
    "    pc3.append(perf)\n",
    "pc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepSIBA MCF7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.520042  0.012489      0.002357   0.473684  0.864972   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.001169         19  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.507482  0.012906      0.029504   0.848485  0.864902   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.003623         33  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.561567  0.012052      0.002834        1.0  0.863622   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.000257          2  ,\n",
       "    Unnamed: 0      cor   mse_all  mse_similars  precision  accuracy  recall  \\\n",
       " 0           0  0.45342  0.013344      0.013714        0.0  0.862518     0.0   \n",
       " \n",
       "    positives  \n",
       " 0          2  ,\n",
       "    Unnamed: 0       cor   mse_all  mse_similars  precision  accuracy  \\\n",
       " 0           0  0.547769  0.011843      0.001306      0.625  0.865989   \n",
       " \n",
       "      recall  positives  \n",
       " 0  0.003266         40  ]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcf7 = []\n",
    "for i in range(5):\n",
    "    perf = pd.read_csv(\"C:/Users/user/Documents/deepSIBA/article_results/supplementary_table_data/sup_table_7/mcf7/fold_%s/ensemble_performance.csv\"%i)\n",
    "    mcf7.append(perf)\n",
    "mcf7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
