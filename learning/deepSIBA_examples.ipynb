{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following GPU devices are available: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "# Check available GPU devices.\n",
    "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : if you encounter an error running any of the examples below consider restarting the kernel and running this cell first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSIBA example 1 : Train ensemble\n",
    "In this example a deepSIBA ensemble model will be trained from scratch using the model_params and train_params dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"max_atoms\" : int(70), \"num_atom_features\" : int(62), \"max_degree\" : int(5), \"num_bond_features\" : int(6),\n",
    "    \"graph_conv_width\" : [128,128,128], \"conv1d_filters\" : int(128), \"conv1d_size\" : int(29), \"dropout_encoder\" : 0.2,\n",
    "    \"conv1d_filters_dist\" : [128,128], \"conv1d_size_dist\" : [17,1], \"dropout_dist\" : 0.2, \"pool_size\" : int(4),\n",
    "    \"dense_size\" : [256,128,128], \"l2reg\" : 0.01, \"dist_thresh\" : 0.2, \"lr\" : 0.001 ,\"ConGauss\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model_params dictionary contains the parameters to build the deepSIBA siamese GCN architecture, more specifically:\n",
    "1. **max_atoms, num_atom_features, max_degree and num_bond_features** refer to the parameters needed to featurize the input chemical structures. For more information, refer to the *ESI of the deepSIBA publication*.\n",
    "2. **graph_conv_width, conv1d_filters, conv1d_size, dropout_encoder** refer to the parameters of the siamese graph encoders.\n",
    "3. **conv1d_filters_dist, conv1d_size_dist, dropout_dist, pool_size, dense_size, l2reg** refer to the parameters of the distance module.\n",
    "4. **dist_thresh** is the distance threshold to consider 2 chemical structures similar in biological effect (needed for custom training metrics).\n",
    "5. **lr** is the learning rate.\n",
    "6. **ConGauss** is by default set to False. **Set it True, only if training becomes difficult due to Loss becoming frequently Inf.** If set to True, a Gaussian Layer constrained to 0 to 1 is used instead of the original Gaussian layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    \"cell_line\" : \"merged_go\", \"split\" : \"5_fold_cv_split\", \"number_folds\" : [0],\n",
    "    \"output_dir\" : \"/home/biolab/Documents/Go distances/deepSIBA/results\",\n",
    "    \"batch_size\" : int(256), \"epochs\" : int(10), \n",
    "    \"N_ensemble\" : int(1), \"nmodel_start\" : int(0), \"prec_threshold\" : 0.2,\n",
    "    \"test_value_norm\" : False,\n",
    "    \"predict_batch_size\":int(2048)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_params dictionary contains the parameters required to train deepSIBA:\n",
    "1. **cell_line** is the cellular model of choice out of **(a375,pc3,vcap,mcf7,merged)** for which we have enough available data. The merged option refers to data merged across cell lines.\n",
    "2. **split** is one of **(train_test_split,5_fold_cv_split,alldata)**. The data to train the models are available in our google drive folder, see **data/readme.md**.\n",
    "3. **number_folds** is a list, if split == train_test_split the number_folds should be [0]. If the split is a 5_fold_cv_split the number_folds should be [0,1,2,3,4] in order to train the model in all splits. If you want to train a model on a specific fold, e.g. the 3rd one, the number_folds should be [2].\n",
    "4. **output_dir** is the full path to the specified output directory.\n",
    "5. **N_ensemble** is the number of models to train and include in the ensemble.\n",
    "6. **nmodel_start** this should be set to 0 if training for the first time, but if training is halted, nmodel_start specifies the model number in the ensemble to start training from.\n",
    "7. **prec_threshold** is the distance threshold to consider 2 chemical structures similar in biological effect (needed for custom training metrics).\n",
    "8. **test_value_norm** is the test/val value already normalized between 0-1.\n",
    "9. **predict_batch_size** is the batch size with which predictions for the pairs of the validation set are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0504 14:59:48.481312 140248729470784 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0504 14:59:48.486895 140248729470784 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0504 14:59:48.487732 140248729470784 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0504 14:59:48.600307 140248729470784 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0504 14:59:48.899194 140248729470784 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0504 14:59:48.959538 140248729470784 deprecation.py:506] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0504 14:59:49.772562 140248729470784 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0504 14:59:50.058641 140248729470784 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0504 14:59:50.062199 140248729470784 deprecation.py:323] From /home/biolab/Documents/Go distances/deepSIBA/learning/utility/gaussian.py:38: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0504 14:59:50.139444 140248729470784 deprecation.py:323] From /home/biolab/Documents/Go distances/deepSIBA/learning/utility/evaluator.py:52: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14787/14787 [==============================] - 1153s 78ms/step - loss: -1.6947 - mean_squared_error: 0.0134 - get_cindex: 0.7306 - r_square: 0.4023 - pearson_r: 0.6543 - mse_similars: nan\n",
      "Epoch 2/10\n",
      "14787/14787 [==============================] - 1153s 78ms/step - loss: -2.1678 - mean_squared_error: 0.0060 - get_cindex: 0.8206 - r_square: 0.7327 - pearson_r: 0.8622 - mse_similars: nan\n",
      "Epoch 3/10\n",
      "14787/14787 [==============================] - 1153s 78ms/step - loss: -2.2939 - mean_squared_error: 0.0049 - get_cindex: 0.8394 - r_square: 0.7824 - pearson_r: 0.8895 - mse_similars: nan\n",
      "Epoch 4/10\n",
      "14787/14787 [==============================] - 1152s 78ms/step - loss: -2.3522 - mean_squared_error: 0.0045 - get_cindex: 0.8481 - r_square: 0.7992 - pearson_r: 0.8986 - mse_similars: 0.0014\n",
      "Epoch 5/10\n",
      "14787/14787 [==============================] - 1152s 78ms/step - loss: -2.3876 - mean_squared_error: 0.0043 - get_cindex: 0.8537 - r_square: 0.8084 - pearson_r: 0.9038 - mse_similars: 0.0013\n",
      "Epoch 6/10\n",
      "14787/14787 [==============================] - 1153s 78ms/step - loss: -2.4140 - mean_squared_error: 0.0041 - get_cindex: 0.8576 - r_square: 0.8152 - pearson_r: 0.9074 - mse_similars: 0.0013\n",
      "Epoch 7/10\n",
      "14787/14787 [==============================] - 1153s 78ms/step - loss: -2.4329 - mean_squared_error: 0.0040 - get_cindex: 0.8603 - r_square: 0.8202 - pearson_r: 0.9101 - mse_similars: 0.0012\n",
      "Epoch 8/10\n",
      "14787/14787 [==============================] - 1153s 78ms/step - loss: -2.4472 - mean_squared_error: 0.0040 - get_cindex: 0.8623 - r_square: 0.8237 - pearson_r: 0.9120 - mse_similars: 0.0012\n",
      "Epoch 9/10\n",
      "14787/14787 [==============================] - 1176s 80ms/step - loss: -2.4607 - mean_squared_error: 0.0039 - get_cindex: 0.8641 - r_square: 0.8265 - pearson_r: 0.9137 - mse_similars: 0.0012\n",
      "Epoch 10/10\n",
      "14787/14787 [==============================] - 1192s 81ms/step - loss: -2.4714 - mean_squared_error: 0.0038 - get_cindex: 0.8655 - r_square: 0.8288 - pearson_r: 0.9149 - mse_similars: 0.0012\n"
     ]
    }
   ],
   "source": [
    "from deepSIBA_train import siba_trainer\n",
    "example_1 = siba_trainer(train_params, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSIBA example 2 : Load trained ensemble and predict\n",
    "In this example a trained deepSIBA ensemble model will be loaded and used to make predictions for the appropriate test set.\n",
    "For each of the cell lines, trained ensembles of either 50 or 10 models for all available splits, can be found in our google drive, see **trained_models/readme.md**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"max_atoms\" : int(70), \"num_atom_features\" : int(62), \"max_degree\" : int(5), \"num_bond_features\" : int(6),\n",
    "    \"graph_conv_width\" : [128,128,128], \"conv1d_filters\" : int(128), \"conv1d_size\" : int(29), \"dropout_encoder\" : 0.2,\n",
    "    \"conv1d_filters_dist\" : [128,128], \"conv1d_size_dist\" : [17,1], \"dropout_dist\" : 0.2, \"pool_size\" : int(4),\n",
    "    \"dense_size\" : [256,128,128], \"l2reg\" : 0.01, \"dist_thresh\" : 0.2, \"lr\" : 0.001 ,\"ConGauss\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all the model is compiled with the parameters (model_params) as described in example 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0504 18:56:56.373223 139938392590144 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0504 18:56:56.378782 139938392590144 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0504 18:56:56.379462 139938392590144 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0504 18:56:56.492802 139938392590144 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0504 18:56:56.778886 139938392590144 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0504 18:56:56.837238 139938392590144 deprecation.py:506] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0504 18:56:57.628402 139938392590144 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0504 18:56:57.902310 139938392590144 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0504 18:56:57.905488 139938392590144 deprecation.py:323] From /home/biolab/Documents/Go distances/deepSIBA/learning/utility/gaussian.py:35: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0504 18:56:57.926778 139938392590144 deprecation.py:323] From /home/biolab/Documents/Go distances/deepSIBA/learning/utility/evaluator.py:52: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "atom_inputs_1 (InputLayer)      (None, 70, 62)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bond_inputs_1 (InputLayer)      (None, 70, 5, 6)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_1 (InputLayer)      (None, 70, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "atom_inputs_2 (InputLayer)      (None, 70, 62)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bond_inputs_2 (InputLayer)      (None, 70, 5, 6)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_2 (InputLayer)      (None, 70, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 42, 128)      694144      atom_inputs_1[0][0]              \n",
      "                                                                 bond_inputs_1[0][0]              \n",
      "                                                                 edge_inputs_1[0][0]              \n",
      "                                                                 atom_inputs_2[0][0]              \n",
      "                                                                 bond_inputs_2[0][0]              \n",
      "                                                                 edge_inputs_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 42, 128)      0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 26, 128)      278528      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 26, 128)      512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 26, 128)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 26, 128)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 26, 128)      16384       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 26, 128)      512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 26, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 26, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 6, 128)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 6, 128)       512         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 768)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 256)          196864      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 256)          1024        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 256)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 128)          32896       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128)          512         dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 128)          16512       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128)          512         dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "main_output (ConGaussianLayer)  [(None, 1), (None, 1 258         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,239,170\n",
      "Trainable params: 1,236,354\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from deepSIBA_model import siamese_model,enc_graph\n",
    "siamese_net=siamese_model(model_params)\n",
    "print(siamese_net.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"architecture.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {\n",
    "    \"cell_line\" : \"merged_go\", \"split\" : \"5_fold_cv_split\", \"fold_id\" : int(0),\n",
    "    \"N_ensemble\" : int(1), \"prec_threshold\" : 0.2,\n",
    "    \"name_pattern\":\"siam_no_augment\",\n",
    "    \"test_value_norm\" : False,\n",
    "    \"predict_batch_size\":2048,\n",
    "    \"to_load\": 2,\n",
    "    \"mu_path\":'/home/biolab/Documents/Go distances/deepSIBA/results/cold/mu/',\n",
    "    \"sigma_path\":'/home/biolab/Documents/Go distances/deepSIBA/results/cold/sigma/',\n",
    "    \"prediction_pattern\":[\"cold_mu_\",\"cold_sigma_\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test_params dictionary contains the parameters required to train deepSIBA:\n",
    "1. **cell_line** is the cellular model of choice out of **(a375,pc3,vcap,mcf7)** for which we have enough available data. Later a merged option will be added.\n",
    "2. **split** is one of **(train_test_split,5_fold_cv_split)**. \n",
    "3. **fold_id** is an integer, if split == train_test_split the fold_id should be 0. If the split is a 5_fold_cv_split the fold_id should be 0,1,2,3 or 4 (one less than the corresponding folder's name for this fold) in order to test the model's performance in **a specific split**.\n",
    "4. **N_ensemble** is the number of total already trained models and at the same time the models included in the ensembled prediction.\n",
    "5. **prec_threshold** is the distance threshold to consider 2 chemical structures similar in biological effect (needed for custom training metrics).\n",
    "6. **name_pattern** is the pattern of the name of files of models' saved weights. **For example** if the weights are saved in files with names such as **siam_no_augment_18.h5** the **pattern is siam_no_augment** .\n",
    "7. **test_value_norm** is the test/val value already normalized between 0-1.\n",
    "8. **predict_batch_size** is the batch size with which predictions for the pairs of the validation set are made.\n",
    "9. **to_load** defines whether trained **models are loaded (1)** and predictions are made or if the **predictions of mean and sigma are loaded directly (give a random integer different from 1).**\n",
    "10. **mu_path** is the path where the mean of distance predictions of each model are. If to_load=1 then its value doesn't matter.\n",
    "11. **sigma_path** is the path where the standard deviation of distance predictions of each model are. If to_load=1 then its value doesn't matter.\n",
    "12. **prediction_pattern** is a list with the pattern of the filenames of mean (mu) and sigma of each model. First is the mean and second the sigma. **THE PREDICTIONS MUST BE SAVED AS .npy files**. **For example** if the pattern is \"cold_mu_\" then the files are like \"cold_mu_**x**.npy\" where x is the model's number\n",
    "\n",
    "**NOTE :** The saved model weights are in the subfolders of \"trained_models/\" and their exact position is described fully given **cell_line** and **split** parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepSIBA_ensembles import siba_val_loader\n",
    "df_cold=siba_val_loader(test_params, model_params,siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ensembled predictions with siba_val_loader function.\n",
    "The data frame with the predictions and the corresponding CV (coefficient of variation) of each prediction is presented below\n",
    "\n",
    "**NOTE:** All distance values have been adjusted in the range 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the model's performance together with a scatterplot of the predicted values VS the true values of the test set, are given bellow:\n",
    "\n",
    "**NOTE:** If there are no predictions lower than the precision threshold defined, the MSE in the predictions lower than the defined threshold and the precision will be **None**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cor   mse_all  mse_similars  precision  accuracy       FPR  positives\n",
      "0  0.498681  0.017472      0.029148   0.300826  0.898095  0.000572        605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utility.evaluator import model_evaluate\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "true = np.array(df_cold.value)\n",
    "pred = np.array(df_cold.mu)\n",
    "get_eval=model_evaluate(pred,true,test_params[\"prec_threshold\"],df_cold)\n",
    "print(get_eval)\n",
    "plt.scatter(pred,true,s = 0.5, label = \"pearson`s r: \"+str(round(get_eval['cor'][0],2)))\n",
    "plt.xlabel(\"predicted GO distance\")\n",
    "plt.ylabel(\"True GO distance\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSIBA example 3 : Screening\n",
    "\n",
    "In this example a trained deepSIBA ensemble model will be used to identify chemical structures from the Chembl and the CMap datasets that affect similar biological processes to a query structure. \n",
    "Given a **query chemical structure and a cellular model**:\n",
    "\n",
    "1. If a structural analogue to the query (ECFP4 similarity > 0.9) is present in the cell line's training set, the **CMap** and the **Chembl** datasets will be screened for chemical structures that affect similar BPs to the query.\n",
    "2. If no structural analogue exists, the appropriate training set from the **CMap** dataset will be screened.\n",
    "\n",
    "**NOTE** : Our trained deepSIBA ensembles allow query structures of up to 60 atoms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"max_atoms\" : int(60), \"num_atom_features\" : int(62), \"max_degree\" : int(5), \"num_bond_features\" : int(6),\n",
    "    \"graph_conv_width\" : [128,128,128], \"conv1d_filters\" : int(128), \"conv1d_size\" : int(29), \"dropout_encoder\" : 0.25,\n",
    "    \"conv1d_filters_dist\" : [128,128], \"conv1d_size_dist\" : [17,1], \"dropout_dist\" : 0.25, \"pool_size\" : int(4),\n",
    "    \"dense_size\" : [256,128,128], \"l2reg\" : 0.01, \"dist_thresh\" : 0.2, \"lr\" : 0.001 ,\"ConGauss\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "screening_params = {\n",
    "    \"query_smile\" : \"CC(C)(C)c1nc(-c2cccc(NS(=O)(=O)c3c(F)cccc3F)c2F)c(-c2ccnc(N)n2)s1\", \n",
    "    \"cell_line\" : \"a375\", \"split\" : \"5_fold_cv_split\" ,\"database\" : [\"Chembl\",\"CMap\"],\n",
    "    \"output_dir\" : \"C:/Users/user/Documents/deepSIBA/results/screening_test1\" , \"model_path\" : \"\", \n",
    "    \"atom_limit\" : int(60), \"N_models\" : int(10),\n",
    "    \"name_pattern\" : \"siam_no_augment\", \"fold_id\" : int(0),\n",
    "    \"screening_threshold\" : 0.22\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The screening_params dictionary contains the parameters required to screen a database with deepSIBA:\n",
    "\n",
    "1. **query_smile** is the smile string of the chemical structure.\n",
    "2. **cell_line** is the cellular model of choice out of **(a375,pc3,vcap,mcf7)** for which we have enough available data. Later a merged option will be added.\n",
    "3. **split** is one of **(train_test_split,5_fold_cv_split,alldata,custom)**. The split selected defines the trained model ensemble that will be loaded. For the screening application the **alldata** split is suggested, where models are trained on the entirety of available data. If **custom** is selected the user must provide a path in **model_path** to load the custom trained models (up to models/ directory).\n",
    "3. **database** is a list of the supported screening databases. So far only **Chembl** and **CMap** are supported.\n",
    "4. **output_dir** full path to the desired output directory to write results. The Chembl screening is performed in parts due to the size of the database.\n",
    "5. **atom_limit** the specified model_params of the trained models, when the split is not **custom** these should be 60.\n",
    "6. **N_ensemble** is the number of total already trained models and at the same time the models included in the ensembled prediction.\n",
    "7. **name_pattern** is the pattern of the name of files of models' saved weights. **For example** if the weights are saved in files with names such as **siam_no_augment_18.h5** the **pattern is siam_no_augment** .\n",
    "8. **fold_id** is an integer, if split == 5_fold_cv_split the fold_id should be 0,1,2,3 or 4 (one less than the corresponding folder's name for this fold), in any other cases the fold_id is not used\n",
    "9. **screening_threshold** only keep hits with predicted distance less than this threshold in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepSIBA_screening import siba_screening\n",
    "siba_screening(screening_params, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
