{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following GPU devices are available: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "# Check available GPU devices.\n",
    "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : if you encounter an error running any of the examples below consider restarting the kernel and running this cell first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSIBA example 1 : Train ensemble\n",
    "In this example a deepSIBA ensemble model will be trained from scratch using the model_params and train_params dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"max_atoms\" : int(60), \"num_atom_features\" : int(62), \"max_degree\" : int(5), \"num_bond_features\" : int(6),\n",
    "    \"graph_conv_width\" : [128,128,128], \"conv1d_filters\" : int(128), \"conv1d_size\" : int(29), \"dropout_encoder\" : 0.25,\n",
    "    \"conv1d_filters_dist\" : [128,128], \"conv1d_size_dist\" : [17,1], \"dropout_dist\" : 0.25, \"pool_size\" : int(4),\n",
    "    \"dense_size\" : [256,128,128], \"l2reg\" : 0.01, \"dist_thresh\" : 0.2, \"lr\" : 0.001 ,\"ConGauss\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model_params dictionary contains the parameters to build the deepSIBA siamese GCN architecture, more specifically:\n",
    "1. **max_atoms, num_atom_features, max_degree and num_bond_features** refer to the parameters needed to featurize the input chemical structures. For more information, refer to the *ESI of the deepSIBA publication*.\n",
    "2. **graph_conv_width, conv1d_filters, conv1d_size, dropout_encoder** refer to the parameters of the siamese graph encoders.\n",
    "3. **conv1d_filters_dist, conv1d_size_dist, dropout_dist, pool_size, dense_size, l2reg** refer to the parameters of the distance module.\n",
    "4. **dist_thresh** is the distance threshold to consider 2 chemical structures similar in biological effect (needed for custom training metrics).\n",
    "5. **lr** is the learning rate.\n",
    "6. **ConGauss** is by default set to False. **Set it True, only if training becomes difficult due to Loss becoming frequently Inf.** If set to True, a Gaussian Layer constrained to 0 to 1 is used instead of the original Gaussian layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    \"cell_line\" : \"a375\", \"split\" : \"5_fold_cv_split\", \"number_folds\" : [0,1,2,3,4],\n",
    "    \"output_dir\" : \"/home/biolab/Documents/Go distances/deepSIBA/results\",\n",
    "    \"batch_size\" : int(128), \"epochs\" : int(20), \n",
    "    \"N_ensemble\" : int(10), \"nmodel_start\" : int(0), \"prec_threshold\" : 0.2,\n",
    "    \"Pre_training\" : False,\n",
    "    \"Pre_trained_cell_dir\" : '/home/biolab/Documents/deepSIBA/learning/trained_models/a375/alldata/models/',\n",
    "    \"pattern_to_load\" : 'siam_no_augment_',\n",
    "    \"model_id_to_load\" : \"20\",\n",
    "    \"test_value_norm\" : False,\n",
    "    \"predict_batch_size\":int(2048)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_params dictionary contains the parameters required to train deepSIBA:\n",
    "1. **cell_line** is the cellular model of choice out of **(a375,pc3,vcap,mcf7,merged)** for which we have enough available data. The merged option refers to data merged across cell lines.\n",
    "2. **split** is one of **(train_test_split,5_fold_cv_split,alldata)**. The data to train the models are available in our google drive folder, see **data/readme.md**.\n",
    "3. **number_folds** is a list, if split == train_test_split the number_folds should be [0]. If the split is a 5_fold_cv_split the number_folds should be [0,1,2,3,4] in order to train the model in all splits. If you want to train a model on a specific fold, e.g. the 3rd one, the number_folds should be [2].\n",
    "4. **output_dir** is the full path to the specified output directory.\n",
    "5. **N_ensemble** is the number of models to train and include in the ensemble.\n",
    "6. **nmodel_start** this should be set to 0 if training for the first time, but if training is halted, nmodel_start specifies the model number in the ensemble to start training from.\n",
    "7. **prec_threshold** is the distance threshold to consider 2 chemical structures similar in biological effect (needed for custom training metrics).\n",
    "8. **Pre_training** indicates whether pre-trained weights will be load.\n",
    "9. **Pre_trained_cell_dir** is the path of the directory that the pre-trained weights are located.\n",
    "10. **pattern_to_load** is the filename pattern of the saved weights.\n",
    "11. **model_id_to_load** is the number/id of the weights of one of the saved models from the ensemble training.\n",
    "12. **test_value_norm** is the test/val value already normalized between 0-1.\n",
    "13. **predict_batch_size** is the batch size with which predictions for the pairs of the validation set are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepSIBA_train import siba_trainer\n",
    "example_1 = siba_trainer(train_params, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSIBA example 2 : Load trained ensemble and predict\n",
    "In this example a trained deepSIBA ensemble model will be loaded and used to make predictions for the appropriate test set.\n",
    "For each of the cell lines, trained ensembles of either 50 or 10 models for all available splits, can be found in our google drive, see **trained_models/readme.md**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"max_atoms\" : int(60), \"num_atom_features\" : int(62), \"max_degree\" : int(5), \"num_bond_features\" : int(6),\n",
    "    \"graph_conv_width\" : [128,128,128], \"conv1d_filters\" : int(128), \"conv1d_size\" : int(29), \"dropout_encoder\" : 0.25,\n",
    "    \"conv1d_filters_dist\" : [128,128], \"conv1d_size_dist\" : [17,1], \"dropout_dist\" : 0.25, \"pool_size\" : int(4),\n",
    "    \"dense_size\" : [256,128,128], \"l2reg\" : 0.01, \"dist_thresh\" : 0.2, \"lr\" : 0.001 ,\"ConGauss\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all the model is compiled with the parameters (model_params) as described in example 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deepSIBA_model import siamese_model,enc_graph\n",
    "siamese_net=siamese_model(model_params)\n",
    "print(siamese_net.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"architecture.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {\n",
    "    \"cell_line\" : \"a375\", \"split\" : \"5_fold_cv_split\", \"fold_id\" : int(2),\n",
    "    \"N_ensemble\" : int(10), \"prec_threshold\" : 0.2,\n",
    "    \"name_pattern\":\"siam_no_augment\",\n",
    "    \"test_value_norm\" : False,\n",
    "    \"predict_batch_size\":2048,\n",
    "    \"to_load\": 2,\n",
    "    \"mu_path\":'/home/biolab/Documents/Go distances/deepSIBA/results/cold/mu/',\n",
    "    \"sigma_path\":'/home/biolab/Documents/Go distances/deepSIBA/results/cold/sigma/',\n",
    "    \"prediction_pattern\":[\"cold_mu_\",\"cold_sigma_\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test_params dictionary contains the parameters required to train deepSIBA:\n",
    "1. **cell_line** is the cellular model of choice out of **(a375,pc3,vcap,mcf7)** for which we have enough available data. Later a merged option will be added.\n",
    "2. **split** is one of **(train_test_split,5_fold_cv_split)**. \n",
    "3. **fold_id** is an integer, if split == train_test_split the fold_id should be 0. If the split is a 5_fold_cv_split the fold_id should be 0,1,2,3 or 4 (one less than the corresponding folder's name for this fold) in order to test the model's performance in **a specific split**.\n",
    "4. **N_ensemble** is the number of total already trained models and at the same time the models included in the ensembled prediction.\n",
    "5. **prec_threshold** is the distance threshold to consider 2 chemical structures similar in biological effect (needed for custom training metrics).\n",
    "6. **name_pattern** is the pattern of the name of files of models' saved weights. **For example** if the weights are saved in files with names such as **siam_no_augment_18.h5** the **pattern is siam_no_augment** .\n",
    "7. **test_value_norm** is the test/val value already normalized between 0-1.\n",
    "8. **predict_batch_size** is the batch size with which predictions for the pairs of the validation set are made.\n",
    "9. **to_load** defines whether trained **models are loaded (1)** and predictions are made or if the **predictions of mean and sigma are loaded directly (give a random integer different from 1).**\n",
    "10. **mu_path** is the path where the mean of distance predictions of each model are. If to_load=1 then its value doesn't matter.\n",
    "11. **sigma_path** is the path where the standard deviation of distance predictions of each model are. If to_load=1 then its value doesn't matter.\n",
    "12. **prediction_pattern** is a list with the pattern of the filenames of mean (mu) and sigma of each model. First is the mean and second the sigma. **THE PREDICTIONS MUST BE SAVED AS .npy files**. **For example** if the pattern is \"cold_mu_\" then the files are like \"cold_mu_**x**.npy\" where x is the model's number\n",
    "\n",
    "**NOTE :** The saved model weights are in the subfolders of \"trained_models/\" and their exact position is described fully given **cell_line** and **split** parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepSIBA_ensembles import siba_val_loader\n",
    "df_cold=siba_val_loader(test_params, model_params,siamese_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ensembled predictions with siba_val_loader function.\n",
    "The data frame with the predictions and the corresponding CV (coefficient of variation) of each prediction is presented below\n",
    "\n",
    "**NOTE:** All distance values have been adjusted in the range 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the model's performance together with a scatterplot of the predicted values VS the true values of the test set, are given bellow:\n",
    "\n",
    "**NOTE:** If there are no predictions lower than the precision threshold defined, the MSE in the predictions lower than the defined threshold and the precision will be **None**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.evaluator import model_evaluate\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "true = np.array(df_cold.value)\n",
    "pred = np.array(df_cold.mu)\n",
    "get_eval=model_evaluate(pred,true,test_params[\"prec_threshold\"],df_cold)\n",
    "print(get_eval)\n",
    "plt.scatter(pred,true,s = 0.5, label = \"pearson`s r: \"+str(round(get_eval['cor'][0],2)))\n",
    "plt.xlabel(\"predicted GO distance\")\n",
    "plt.ylabel(\"True GO distance\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSIBA example 3 : Screening\n",
    "\n",
    "In this example a trained deepSIBA ensemble model will be used to identify chemical structures from the Chembl and the CMap datasets that affect similar biological processes to a query structure. \n",
    "Given a **query chemical structure and a cellular model**:\n",
    "\n",
    "1. If a structural analogue to the query (ECFP4 similarity > 0.9) is present in the cell line's training set, the **CMap** and the **Chembl** datasets will be screened for chemical structures that affect similar BPs to the query.\n",
    "2. If no structural analogue exists, the appropriate training set from the **CMap** dataset will be screened.\n",
    "\n",
    "**NOTE** : Our trained deepSIBA ensembles allow query structures of up to 60 atoms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"max_atoms\" : int(60), \"num_atom_features\" : int(62), \"max_degree\" : int(5), \"num_bond_features\" : int(6),\n",
    "    \"graph_conv_width\" : [128,128,128], \"conv1d_filters\" : int(128), \"conv1d_size\" : int(29), \"dropout_encoder\" : 0.25,\n",
    "    \"conv1d_filters_dist\" : [128,128], \"conv1d_size_dist\" : [17,1], \"dropout_dist\" : 0.25, \"pool_size\" : int(4),\n",
    "    \"dense_size\" : [256,128,128], \"l2reg\" : 0.01, \"dist_thresh\" : 0.2, \"lr\" : 0.001 ,\"ConGauss\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "screening_params = {\n",
    "    \"query_smile\" : \"CC(C)(C)c1nc(-c2cccc(NS(=O)(=O)c3c(F)cccc3F)c2F)c(-c2ccnc(N)n2)s1\", \n",
    "    \"cell_line\" : \"a375\", \"split\" : \"5_fold_cv_split\" ,\"database\" : [\"Chembl\",\"CMap\"],\n",
    "    \"output_dir\" : \"C:/Users/user/Documents/deepSIBA/results/screening_test1\" , \"model_path\" : \"\", \n",
    "    \"atom_limit\" : int(60), \"N_models\" : int(10),\n",
    "    \"name_pattern\" : \"siam_no_augment\", \"fold_id\" : int(0),\n",
    "    \"screening_threshold\" : 0.22\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The screening_params dictionary contains the parameters required to screen a database with deepSIBA:\n",
    "\n",
    "1. **query_smile** is the smile string of the chemical structure.\n",
    "2. **cell_line** is the cellular model of choice out of **(a375,pc3,vcap,mcf7)** for which we have enough available data. Later a merged option will be added.\n",
    "3. **split** is one of **(train_test_split,5_fold_cv_split,alldata,custom)**. The split selected defines the trained model ensemble that will be loaded. For the screening application the **alldata** split is suggested, where models are trained on the entirety of available data. If **custom** is selected the user must provide a path in **model_path** to load the custom trained models (up to models/ directory).\n",
    "3. **database** is a list of the supported screening databases. So far only **Chembl** and **CMap** are supported.\n",
    "4. **output_dir** full path to the desired output directory to write results. The Chembl screening is performed in parts due to the size of the database.\n",
    "5. **atom_limit** the specified model_params of the trained models, when the split is not **custom** these should be 60.\n",
    "6. **N_ensemble** is the number of total already trained models and at the same time the models included in the ensembled prediction.\n",
    "7. **name_pattern** is the pattern of the name of files of models' saved weights. **For example** if the weights are saved in files with names such as **siam_no_augment_18.h5** the **pattern is siam_no_augment** .\n",
    "8. **fold_id** is an integer, if split == 5_fold_cv_split the fold_id should be 0,1,2,3 or 4 (one less than the corresponding folder's name for this fold), in any other cases the fold_id is not used\n",
    "9. **screening_threshold** only keep hits with predicted distance less than this threshold in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepSIBA_screening import siba_screening\n",
    "siba_screening(screening_params, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
