{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egYX2aATTDiK"
   },
   "source": [
    "# Load libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubBgA0SXLYnl"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from comet_ml import Experiment\n",
    "import numpy as np\n",
    "from numpy import inf, ndarray\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import re\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model, Model\n",
    "from tempfile import TemporaryFile\n",
    "from keras import layers\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors\n",
    "from matplotlib import pyplot as plt\n",
    "# matplotlib inline\n",
    "from keras.callbacks import History, ReduceLROnPlateau\n",
    "from keras.layers import Input, BatchNormalization, Activation\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Dropout, Layer\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.regularizers import l2\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from copy import deepcopy\n",
    "from NGF.utils import filter_func_args, mol_shapes_to_dims\n",
    "import NGF.utils\n",
    "import NGF_layers.features\n",
    "import NGF_layers.graph_layers\n",
    "from NGF_layers.features import one_of_k_encoding, one_of_k_encoding_unk, atom_features, bond_features, num_atom_features, num_bond_features\n",
    "from NGF_layers.features import padaxis, tensorise_smiles, concat_mol_tensors\n",
    "from NGF_layers.graph_layers import temporal_padding, neighbour_lookup, NeuralGraphHidden, NeuralGraphOutput\n",
    "from math import ceil\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from utility.gaussian import GaussianLayer\n",
    "from utility.gaussian import custom_loss\n",
    "from utility.evaluator import r_square, get_cindex, pearson_r, mse_sliced, model_evaluate\n",
    "from utility.Generator import train_generator\n",
    "from model import enc_mols, distance_module\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "# Check available GPU devices.\n",
    "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRWlUdzxUJ3e"
   },
   "source": [
    "# Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckmCcQT3UMNz"
   },
   "outputs": [],
   "source": [
    "#Load unique smiles and tensorize them\n",
    "smiles = pd.read_csv('data/mcf7q1/can_smiles_all.csv',index_col=0)\n",
    "\n",
    "X_atoms, X_bonds, X_edges = tensorise_smiles(smiles.x, max_degree=5, max_atoms = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwZGzl08UM33"
   },
   "outputs": [],
   "source": [
    "smiles=list(smiles['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train data\n",
    "df = pd.read_csv('data/mcf7q1/80dr_cold/train_30_7.csv',index_col=0)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2aaBIDDT4NZ"
   },
   "outputs": [],
   "source": [
    "num_molecules=len(df)\n",
    "max_atoms = 60\n",
    "max_degree = 5\n",
    "num_atom_features = 62\n",
    "num_bond_features = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvZDS2MDT10K"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVArTy7aT3DZ"
   },
   "outputs": [],
   "source": [
    "p = {'lr': 0.0001,\n",
    "     'nfilters': int(32),\n",
    "     'size': int(8),\n",
    "     'conv_width' : 128,\n",
    "     'fp_length' : 256,\n",
    "     'size_drug_1' : 8,\n",
    "     'size_drug_2' : 4,\n",
    "     'size_protein_1' : 8,\n",
    "     'size_protein_2' : 16,\n",
    "     'size_protein_3' : 3,\n",
    "     'batch_size': int(128),\n",
    "     'dense_size': int(256),\n",
    "     'dense_size_2': 512,\n",
    "     'dropout': 0.25,\n",
    "     'l2reg': 0.01}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcQJfqsgUbLv"
   },
   "source": [
    "# Load Cold validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JXhbFznUfce"
   },
   "outputs": [],
   "source": [
    "df_cold = pd.read_csv('data/mcf7q1/80dr_cold/cold_val_new.csv',index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLpl-JH_U0SL"
   },
   "outputs": [],
   "source": [
    "smiles_cold = df_cold['rdkit.x']\n",
    "smiles_cold2 = df_cold['rdkit.y']\n",
    "X_atoms_cold_1, X_bonds_cold_1, X_edges_cold_1 = tensorise_smiles(smiles_cold, max_degree=5, max_atoms = 60)\n",
    "X_atoms_cold_2, X_bonds_cold_2, X_edges_cold_2 = tensorise_smiles(smiles_cold2, max_degree=5, max_atoms = 60)\n",
    "Y_cold = df_cold.value\n",
    "Y_cold = Y_cold/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38w8Y_DzU1XW"
   },
   "source": [
    "# Fit ensembles (WARNING: Training is difficult in some cases due to the number of dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9NOXgvhU3tH"
   },
   "outputs": [],
   "source": [
    "cold_preds_mus=[]\n",
    "cold_preds_sigmas=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUd8VKWjU51T"
   },
   "outputs": [],
   "source": [
    "for n in range(50):\n",
    "    \n",
    "    # Initialize model\n",
    "    atoms0_1 = Input(name='atom_inputs_1', shape=(max_atoms, num_atom_features),dtype = 'float32')\n",
    "    bonds_1 = Input(name='bond_inputs_1', shape=(max_atoms, max_degree, num_bond_features),dtype = 'float32')\n",
    "    edges_1 = Input(name='edge_inputs_1', shape=(max_atoms, max_degree), dtype='int32')\n",
    "\n",
    "    atoms0_2 = Input(name='atom_inputs_2', shape=(max_atoms, num_atom_features),dtype = 'float32')\n",
    "    bonds_2 = Input(name='bond_inputs_2', shape=(max_atoms, max_degree, num_bond_features),dtype = 'float32')\n",
    "    edges_2 = Input(name='edge_inputs_2', shape=(max_atoms, max_degree), dtype='int32')\n",
    "    \n",
    "    # Initialize encoder\n",
    "    encoder_mols = enc_mols(p, 0.001, 128, 256,max_atoms, num_atom_features,max_degree, num_bond_features)\n",
    "    encoded_1 = encoder_mols([atoms0_1,bonds_1,edges_1])\n",
    "    encoded_2 = encoder_mols([atoms0_2,bonds_2,edges_2])\n",
    "\n",
    "    #Use distance module to calculate the final distance features vector\n",
    "    distance_net=distance_module(p,encoded_1,encoded_2,max_atoms, num_atom_features,max_degree, num_bond_features)\n",
    "    \n",
    "    #Final Gaussian Layer to predict mean distance and standard deaviation of distance\n",
    "    mu, sigma = GaussianLayer(1, name='main_output')(distance_net)\n",
    "    siamese_net = Model(inputs=[atoms0_1,bonds_1,edges_1,atoms0_2,bonds_2,edges_2],outputs=mu)\n",
    "    print(siamese_net.summary())\n",
    "    \n",
    "    #Compile model\n",
    "    thresh=0.20 #threshold to consider similars\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)        \n",
    "    siamese_net.compile(optimizer= adam,loss= custom_loss(sigma),metrics=['mse', get_cindex, r_square, pearson_r, mse_sliced(thresh)])\n",
    "    \n",
    "    \n",
    "    #Load Weights if you want to use pretrained models\n",
    "    #siamese_net.load_weights('models/Nick_ensembles_no_augm/models/siam_ens_10.h5')\n",
    "    \n",
    "    # Augment dataset (if wanted)\n",
    "    #df_new = augment(df_rest,df,1)\n",
    "    \n",
    "    # Train with fitgen\n",
    "    rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=2, min_lr=0.00001, verbose=1, min_delta=1e-5)\n",
    "    term=keras.callbacks.TerminateOnNaN()\n",
    "    bs=128\n",
    "    NUM_EPOCHS = 10\n",
    "    #df_new = df_new.sample(frac=1).reset_index(drop=True)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    #Set total number of training samples and tests samples\n",
    "    NUM_TRAIN = len(df)\n",
    "    trainGen=train_generator(bs,df,smiles,X_atoms, X_bonds, X_edges)\n",
    "    siamese_net.fit_generator(trainGen,\n",
    "                        steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        verbose = 1,\n",
    "                        shuffle = True,\n",
    "                        callbacks= [term, rlr])\n",
    "    #validation_data = ([X_atoms_cold_1,X_bonds_cold_1,X_edges_cold_1,X_atoms_cold_2, X_bonds_cold_2, X_edges_cold_2],Y_cold),\n",
    "    # Save model\n",
    "    siamese_net.save_weights('testing_train/models/siam_no_augment_%s.h5'%n)\n",
    "    # Decouple model at the gaussian\n",
    "    Gauss = keras.Model(siamese_net.inputs, siamese_net.get_layer('main_output').output)\n",
    "    # evaluate and save evals\n",
    "    y_pred = siamese_net.predict([X_atoms_cold_1,X_bonds_cold_1,X_edges_cold_1,X_atoms_cold_2, X_bonds_cold_2, X_edges_cold_2],batch_size=2048)\n",
    "    get = model_evaluate(y_pred,Y_cold,thresh,df_cold)\n",
    "    get.to_csv('testing_train/performance/Model_No_%s.csv'%n)\n",
    "    # Predict on cold\n",
    "    cold_pred = Gauss.predict([X_atoms_cold_1,X_bonds_cold_1,X_edges_cold_1,X_atoms_cold_2, X_bonds_cold_2, X_edges_cold_2],batch_size=2048)\n",
    "    # Append mus and sigmas and save at the same time\n",
    "    cold_preds_mus.append(cold_pred[0])\n",
    "    np.save('testing_train/cold/mu/cold_mu_No_%s.npy'%n, cold_pred[0])\n",
    "    cold_preds_sigmas.append(cold_pred[1])\n",
    "    np.save('testing_train/cold/sigma/cold_sigma_No_%s.npy'%n, cold_pred[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
